查找表达式系统和规则执行器的相关代码：


[4 tools called]


查看关键文件以理解这两个系统：


[3 tools called]


查看规则执行器的执行逻辑：


[2 tools called]


## Spark SQL 表达式系统与规则执行器详解

### 一、表达式系统（Expression System）

#### 1. 核心概念

表达式系统是 Catalyst 优化器的基础，用于表示查询中的计算逻辑。

```scala
abstract class Expression extends TreeNode[Expression] {
  // 1. 数据类型
  def dataType: DataType
  
  // 2. 可空性
  def nullable: Boolean
  
  // 3. 确定性（是否对相同输入产生相同输出）
  lazy val deterministic: Boolean = children.forall(_.deterministic)
  
  // 4. 可折叠性（是否可以在编译时求值）
  def foldable: Boolean = false
  
  // 5. 解析状态
  lazy val resolved: Boolean = childrenResolved && checkInputDataTypes().isSuccess
  
  // 6. 求值方法
  def eval(input: InternalRow = null): Any
  
  // 7. 代码生成
  def genCode(ctx: CodegenContext): ExprCode
}
```

#### 2. 表达式层次结构

```
Expression (抽象基类)
├── LeafExpression (叶子表达式，无子节点)
│   ├── Literal (字面量)
│   ├── AttributeReference (属性引用)
│   └── UnresolvedAttribute (未解析属性)
│
├── UnaryExpression (一元表达式)
│   ├── Cast (类型转换)
│   ├── Not (逻辑非)
│   └── UnaryMinus (一元负号)
│
├── BinaryExpression (二元表达式)
│   ├── Add (加法)
│   ├── Subtract (减法)
│   ├── Multiply (乘法)
│   ├── EqualTo (等于)
│   └── And (逻辑与)
│
└── NamedExpression (命名表达式)
    ├── AttributeReference
    └── Alias (别名)
```

#### 3. 表达式特性

**a) 树形结构**
```scala
// 表达式树示例: (a + b) * 2
Multiply(
  Add(AttributeReference("a"), AttributeReference("b")),
  Literal(2)
)
```

**b) 惰性求值属性**
```scala
// 这些属性是惰性计算的
lazy val deterministic: Boolean = children.forall(_.deterministic)
lazy val resolved: Boolean = childrenResolved && checkInputDataTypes().isSuccess
```

**c) 代码生成**
```scala
// 生成 Java 代码用于 Whole-Stage Code Generation
def genCode(ctx: CodegenContext): ExprCode = {
  val isNull = ctx.freshName("isNull")
  val value = ctx.freshName("value")
  // 生成代码...
}
```

**d) 解释执行**
```scala
// 用于解释模式执行
def eval(input: InternalRow = null): Any = {
  // 对输入行求值
}
```

#### 4. 表达式示例

```scala
// SQL: SELECT (age + 1) * 2 AS doubled_age FROM users

// 表达式树表示
Alias(
  name = "doubled_age",
  child = Multiply(
    Add(AttributeReference("age"), Literal(1)),
    Literal(2)
  )
)
```

### 二、规则执行器（Rule Executor）

#### 1. 核心架构

`RuleExecutor` 是 Catalyst 优化器的执行引擎，负责按批次执行规则。

```scala
abstract class RuleExecutor[TreeType <: TreeNode[_]] {
  // 定义规则批次
  def batches: Seq[Batch]
  
  // 执行规则
  def execute(plan: TreeType): TreeType
}
```

#### 2. 批次（Batch）结构

```scala
case class Batch(
    name: String,              // 批次名称
    strategy: Strategy,        // 执行策略
    rules: Rule[TreeType]*    // 规则列表
)

// 执行策略
abstract class Strategy {
  def maxIterations: Int      // 最大迭代次数
  def errorOnExceed: Boolean  // 超过最大次数是否报错
}

// 常用策略
object Once extends Strategy { val maxIterations = 1 }  // 执行一次
case class FixedPoint(maxIterations: Int) extends Strategy  // 固定点迭代
```

#### 3. 执行流程

```scala
def execute(plan: TreeType): TreeType = {
  var curPlan = plan
  
  // 遍历每个批次
  batches.foreach { batch =>
    var iteration = 1
    var lastPlan = curPlan
    var continue = true
    
    // 固定点迭代：直到计划不再变化或达到最大迭代次数
    while (continue) {
      // 在批次内顺序执行规则
      curPlan = batch.rules.foldLeft(curPlan) {
        case (plan, rule) =>
          val result = rule(plan)  // 应用规则
          
          // 记录规则执行效果
          if (!result.fastEquals(plan)) {
            planChangeLogger.logRule(rule.ruleName, plan, result)
          }
          
          result
      }
      
      iteration += 1
      
      // 检查是否达到固定点（计划不再变化）
      if (curPlan.fastEquals(lastPlan)) {
        continue = false  // 收敛，停止迭代
      }
      
      // 检查是否超过最大迭代次数
      if (iteration > batch.strategy.maxIterations) {
        continue = false
      }
      
      lastPlan = curPlan
    }
  }
  
  curPlan
}
```

#### 4. 规则接口

```scala
abstract class Rule[TreeType <: TreeNode[_]] {
  def ruleName: String
  def apply(plan: TreeType): TreeType
}
```

#### 5. 实际应用示例

**Analyzer 中的规则批次：**
```scala
class Analyzer extends RuleExecutor[LogicalPlan] {
  override def batches: Seq[Batch] = Seq(
    Batch("Substitution", fixedPoint,
      CTESubstitution,
      WindowsSubstitution,
      EliminateUnions
    ),
    Batch("Resolution", fixedPoint,
      ResolveCatalogs,
      ResolveRelations,
      ResolveReferences,
      ResolveFunctions,
      ...
    ),
    Batch("Post-Hoc Resolution", Once,
      ResolveCommandsWithIfExists
    )
  )
}
```

**Optimizer 中的规则批次：**
```scala
class Optimizer extends RuleExecutor[LogicalPlan] {
  override def batches: Seq[Batch] = Seq(
    Batch("Finish Analysis", Once,
      EliminateResolvedHint
    ),
    Batch("Operator Optimization before Inferring Filters", fixedPoint,
      PushDownPredicates,
      ColumnPruning,
      ...
    ),
    Batch("Infer Filters", Once,
      InferFiltersFromConstraints
    )
  )
}
```

### 三、两者交互关系

#### 1. 表达式在规则中的应用

```scala
// 规则示例：常量折叠
object ConstantFolding extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = {
    plan.transformExpressions {
      // 如果表达式是可折叠的，在编译时求值
      case expr if expr.foldable =>
        Literal(expr.eval(), expr.dataType)
    }
  }
}

// 示例：1 + 2 会被折叠为 Literal(3)
```

#### 2. 规则对表达式的转换

```scala
// 规则：类型转换
object ImplicitTypeCasts extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = {
    plan.transformExpressions {
      case e if !e.resolved => e  // 跳过未解析的
      case e if e.childrenResolved && !e.resolved =>
        // 尝试类型转换
        TypeCoercion.implicitCast(e, expectedType)
    }
  }
}
```

### 四、关键设计模式

#### 1. 访问者模式
- 表达式树遍历：`transformUp`、`transformDown`
- 规则应用：规则作为访问者访问计划树

#### 2. 策略模式
- 执行策略：`Once`、`FixedPoint`
- 规则选择：不同批次使用不同策略

#### 3. 模板方法模式
- `RuleExecutor.execute()` 定义执行框架
- 子类定义具体的 `batches`

### 五、性能优化特性

#### 1. 固定点迭代
- 规则重复执行直到计划不再变化
- 确保规则间的依赖关系得到满足

#### 2. 规则剪枝
- 使用 `TreePattern` 快速判断是否需要应用规则
- 避免不必要的树遍历

#### 3. 计划缓存
- 使用 `fastEquals` 快速比较计划
- 避免重复计算

### 六、总结

| 特性     | 表达式系统       | 规则执行器   |
| -------- | ---------------- | ------------ |
| **作用** | 表示计算逻辑     | 执行计划转换 |
| **结构** | 树形结构         | 批次+规则    |
| **执行** | 解释/代码生成    | 固定点迭代   |
| **状态** | 不可变           | 不可变       |
| **遍历** | transformUp/Down | 递归应用规则 |

两者共同构成了 Catalyst 优化器的核心：
- 表达式系统：表示查询的计算逻辑
- 规则执行器：执行计划的转换和优化

这种设计使得 Spark SQL 能够：
1. 灵活地表示复杂的查询逻辑
2. 高效地应用优化规则
3. 支持扩展新的表达式和规则

如需深入某个具体方面，可以继续提问。